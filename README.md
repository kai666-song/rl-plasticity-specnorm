<h1 align="center">ğŸ§  Mitigating Plasticity Loss in Deep RL via Spectral Normalization</h1>

<p align="center">
  <b>åŸºäºè°±å½’ä¸€åŒ–çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ å¯å¡‘æ€§ä¸¢å¤±ç¼“è§£ç ”ç©¶</b><br>
  <i>Course Design Project | å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹è®¾è®¡</i>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Python-3.10+-blue.svg" alt="Python">
  <img src="https://img.shields.io/badge/PyTorch-2.0+-red.svg" alt="PyTorch">
  <img src="https://img.shields.io/badge/License-MIT-green.svg" alt="License">
</p>

---

## ğŸ“Œ TL;DR

> **Spectral Normalization achieves +20% reward improvement and reduces dead neurons by 52% compared to baseline, outperforming all other methods including LayerNorm, ReDo, and activation function modifications.**

| Method | Test Reward | Dead Units | vs Baseline |
|:-------|:-----------:|:----------:|:-----------:|
| Baseline (ReLU) | 5.80 | 82.4% | - |
| LayerNorm | 4.65 | 75.9% | -19.8% âŒ |
| ReDo Reset | 5.73 | 71.4% | -1.1% |
| **Spectral Norm** | **6.96** | **39.5%** | **+20.0%** âœ… |

---

## ğŸš€ How to Reproduce

### Dependencies

| Package | Version |
|:--------|:--------|
| Python | 3.10+ |
| PyTorch | 2.0+ |
| NumPy | 1.24+ |
| Matplotlib | 3.7+ |
| ProcGen | 0.10.7 |
| TensorBoard | 2.13+ |

### Installation

```bash
# Create conda environment
conda create -n rlcourse python=3.10
conda activate rlcourse

# Install PyTorch (CUDA 12.1)
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Install dependencies
pip install -r requirements.txt
```

### Quick Start (Single Seed)

```bash
# Run Spectral Norm experiment (recommended)
python train.py -p hyperparams_quick.yaml -c specnorm -n specnorm_exp

# Run Baseline experiment
python train.py -p hyperparams_quick.yaml -c baseline -n baseline_exp

# Run ReDo experiment
python train.py -p hyperparams_quick.yaml -c redo -n redo_exp

# Resume from checkpoint
python train.py -p hyperparams_quick.yaml -n <exp_name> -r
```

### Multi-Seed Training (Recommended for Publication)

```bash
# Linux/Mac
./run_seeds.sh specnorm 5    # Run Spectral Norm with 5 seeds
./run_seeds.sh baseline 5    # Run Baseline with 5 seeds
./run_seeds.sh redo 5        # Run ReDo with 5 seeds
./run_seeds.sh all 5         # Run all methods with 5 seeds

# Windows
run_seeds.bat specnorm 5
run_seeds.bat baseline 5
```

### Analysis & Visualization

```bash
# Generate comparison plots (single-seed and multi-seed)
python plot_comparison.py

# Analyze feature representations (SVD, dead neurons)
python analyze_features.py

# Analyze layer norms (check for gradient vanishing)
python analyze_layer_norms.py

# Verify Power Iteration accuracy
python analyze_power_iteration.py
```

---

## ğŸ¯ Research Question

**How can we prevent plasticity loss (feature rank collapse & dead neurons) in deep reinforcement learning while maintaining training stability?**

### Key Findings

1. **"Keeping neurons alive" â‰  "Effective learning"**: Leaky ReLU eliminates dead neurons (0%) but decreases reward by 15%
2. **Reset mechanisms are band-aids**: ReDo works but introduces training instability (sawtooth curves)
3. **Spectral Normalization is the principled solution**: Mathematically constrains Lipschitz constant, preventing rank collapse

---

## ğŸ“Š Results

### Performance Comparison

![Summary Comparison](results/comparison_figures/summary_comparison.png)

### Feature Analysis (Using Real Environment Data)

We analyze features using **2,560 real ProcGen observations** (not Gaussian noise!) to ensure N â‰¥ 10Ã—D for valid SVD computation.

![Singular Value Spectrum](results/feature_analysis/singular_value_spectrum_real.png)

| Method | Dead Neurons* | Avg Activation Rate |
|:-------|:-------------:|:-------------------:|
| Baseline | 19.14% | 21.52% |
| ReDo | 26.17% | 32.53% |
| **Spectral Norm** | 25.39% | **66.76%** |

> *Dead units are defined as neurons that **never activate** over the entire test set (2.5k steps), distinguishing true neuron death from normal ReLU sparsity.

---

## ğŸ”¬ Methodology

### Spectral Normalization

We apply Spectral Normalization to the **shared encoder only** (not the value head):

$$W_{SN} = \frac{W}{\sigma(W)}$$

where $\sigma(W)$ is the largest singular value of $W$.

**Training vs Evaluation:**
- **Training**: Uses Power Iteration (1 iteration) for efficient approximation of $\sigma(W)$
- **Evaluation**: Uses exact SVD computation for accurate analysis

This distinction is important for reproducibility. Power Iteration is sufficient for training but exact SVD should be used for final analysis.

**Why not apply SN to Value Network?**

The value function $V(s)$ can have large magnitude (e.g., cumulative reward > 10). Constraining Lipschitz constant â‰¤ 1 would cause:

$$|V(s_1) - V(s_2)| \leq \|s_1 - s_2\|$$

This leads to **Value Underestimation Bias**, destabilizing policy gradients.

### Experimental Setup

| Parameter | Value |
|:----------|:------|
| Environment | ProcGen CoinRun |
| Algorithm | PPO |
| Training Epochs | 3,000 |
| Task Shift Points | [1000, 2000] |
| Hidden Size | 256 |
| Learning Rate | 0.0005 |
| Batch Size | 64 |
| Buffer Size | 1024 |

---

## ğŸ“ˆ Ablation Studies

| Method | Principle | Reward | Dead Units | Verdict |
|:-------|:----------|:------:|:----------:|:--------|
| Baseline | ReLU | 5.80 | 82.4% | Reference |
| Leaky ReLU | Negative slope | 4.94 | 0.0% | âŒ Alive but useless |
| Mish | Smooth activation | 5.72 | 93.6% | âŒ Worse |
| LayerNorm | Normalization | 4.65 | 75.9% | âŒ Industry standard fails |
| RMSNorm | Lightweight norm | 4.21 | 67.4% | âŒ Worst |
| ReDo | Periodic reset | 5.73 | 71.4% | âš ï¸ Unstable |
| **Spectral Norm** | Lipschitz constraint | **6.96** | **39.5%** | âœ… **Best** |

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ train.py                     # Training entry point
â”œâ”€â”€ run_seeds.sh/.bat            # Multi-seed training scripts
â”œâ”€â”€ analyze_features.py          # Feature analysis (SVD, dead neurons)
â”œâ”€â”€ analyze_layer_norms.py       # Layer norm analysis (gradient vanishing check)
â”œâ”€â”€ analyze_power_iteration.py   # Power Iteration vs SVD comparison
â”œâ”€â”€ plot_comparison.py           # Generate comparison figures
â”‚
â”œâ”€â”€ algos/ppo/
â”‚   â”œâ”€â”€ model.py                 # PPO model with Spectral Norm support
â”‚   â””â”€â”€ trainer.py               # PPO trainer (optimized SVD computation)
â”‚
â”œâ”€â”€ shared/
â”‚   â”œâ”€â”€ modules.py               # Network modules (ConvEncoder, SN, etc.)
â”‚   â”œâ”€â”€ runner.py                # Experiment runner
â”‚   â”œâ”€â”€ trainer.py               # Base trainer
â”‚   â””â”€â”€ plotting.py              # Plotting utilities
â”‚
â”œâ”€â”€ envs/
â”‚   â””â”€â”€ mdps.py                  # ProcGen environment wrapper
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ comparison_figures/      # Main result figures
â”‚   â”œâ”€â”€ feature_analysis/        # SVD and activation analysis
â”‚   â””â”€â”€ multiseed/               # Multi-seed experiment results
â”‚
â”œâ”€â”€ hyperparams.yaml             # Full experiment config
â”œâ”€â”€ hyperparams_quick.yaml       # Quick experiment config (3000 epochs)
â”œâ”€â”€ hyperparams_multiseed.yaml   # Multi-seed experiment config
â””â”€â”€ requirements.txt             # Python dependencies
```

---

## ğŸ”‘ Key Implementation Details

### 1. Optimized SVD Computation

```python
# SVD is only computed at logging intervals (every 10 epochs)
# This significantly reduces training overhead
should_compute_diagnostics = (epoch % 10 == 0)
```

### 2. Batch Size Validation for Effective Rank

```python
# Effective rank is only valid when N >= D
# Training batch_size (64) < h_size (256) produces invalid results
# Use analyze_features.py with N >= 2560 for valid analysis
if N < D * min_ratio:
    return torch.tensor(-1.0)  # Placeholder for invalid computation
```

### 3. Explicit Input Dimension Validation

```python
# ConvEncoder validates input dimensions explicitly
# No silent auto-correction that could mask bugs
if x.shape[1] != expected_flat_size:
    raise ValueError(f"Expected shape (B, {expected_flat_size}), got {x.shape}")
```

### 4. Real Environment Data for SVD

```python
# Use real ProcGen observations, NOT Gaussian noise!
# Ensure N >= 10*D for valid singular value spectrum
observations = collect_real_observations(num_samples=2560)  # D=256
```

---

## ğŸ“š References

```bibtex
@article{dohare2024plasticity,
  title={A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning},
  author={Dohare, Shibhansh and others},
  journal={arXiv preprint arXiv:2405.19153},
  year={2024}
}

@inproceedings{miyato2018spectral,
  title={Spectral Normalization for Generative Adversarial Networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  booktitle={ICLR},
  year={2018}
}

@article{kumar2020implicit,
  title={Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning},
  author={Kumar, Aviral and others},
  journal={arXiv preprint arXiv:2010.14498},
  year={2020}
}
```

---

## ğŸ“„ License

MIT License - feel free to use this code for your research!

---

<p align="center">
  <i>Made with â¤ï¸ for Deep Reinforcement Learning</i><br>
  <b>If you find this useful, please â­ star this repo!</b>
</p>
