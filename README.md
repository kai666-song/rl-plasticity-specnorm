<h1 align="center">åŸºäºè°±å½’ä¸€åŒ–çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ å¯å¡‘æ€§ä¸¢å¤±ç ”ç©¶</h1>

<p align="center">
  <b>Deep RL Plasticity Loss Study with Spectral Normalization</b><br>
  <i>è¯¾ç¨‹è®¾è®¡æŠ¥å‘Š | Course Design Project</i>
</p>

---

## ğŸ¯ æ ¸å¿ƒå‘ç° (Key Finding)

> **"We demonstrate that Spectral Normalization effectively prevents feature rank collapse in deep RL, achieving +20% reward improvement and reducing dead neurons by 52%."**

é€šè¿‡ç³»ç»Ÿæ€§å®éªŒå¯¹æ¯”ï¼Œæˆ‘ä»¬éªŒè¯äº† **Spectral Normalizationï¼ˆè°±å½’ä¸€åŒ–ï¼‰** åœ¨ç¼“è§£æ·±åº¦å¼ºåŒ–å­¦ä¹ å¯å¡‘æ€§ä¸¢å¤±é—®é¢˜ä¸Šçš„æ˜¾è‘—æ•ˆæœã€‚

**High-Level Conclusion**: Our experiments demonstrate that Spectral Normalization outperforms heuristic resetting methods (ReDo) by maintaining high feature rank stability without sacrificing training stability. Unlike activation function modifications (Leaky ReLU) that merely "keep neurons alive" without improving feature quality, SN achieves an **effective balance between Stability and Plasticity**.

> **Note on Value Network**: We intentionally do not apply Spectral Normalization to the value network, as constraining its output range could limit its ability to predict high-magnitude rewards accurately.

## â­ Highlights 

| Method | Test Reward | Stability | Dead Units | Feature Rank |
|:-------|:-----------:|:---------:|:----------:|:------------:|
| Baseline (ReLU) | 5.80 | Medium | 82.4% (Collapse) | Low |
| LayerNorm | 4.65 âŒ | Medium | 75.9% | Low |
| Leaky ReLU | 4.94 âŒ | Medium | 0.0% (Alive but useless) | Low |
| ReDo Reset | 5.73 | âš ï¸ Unstable | 71.4% (Recycled) | Medium |
| **Spectral Norm (Ours)** | **6.96** âœ… | **Stable** | **39.5%** (Healthy) | **High** |

## ğŸ“Š å®éªŒç»“æœå¯¹æ¯” (Results Comparison)

![Summary Comparison](results/comparison_figures/summary_comparison.png)

### å®šé‡ç»“æœ (Quantitative Results)

| æ–¹æ³• | Test Reward | Dead Units | è¯´æ˜ |
|:-----|:-----------:|:----------:|:-----|
| Baseline (ReLU) | 5.80 | 82.4% | åŸå§‹æ–¹æ³•ï¼Œæ­»ç¥ç»å…ƒæ¯”ä¾‹é«˜ |
| **Spectral Norm (Ours)** | **6.96** | **39.5%** | **+20% reward, -52% dead units** |

### å…³é”®æ´å¯Ÿ (Key Insights)

1. **"ä¿æ´»"â‰ "æœ‰æ•ˆ"**ï¼šLeaky ReLU æ¶ˆé™¤äº†æ­»ç¥ç»å…ƒï¼ˆ0%ï¼‰ï¼Œä½† Reward åè€Œä¸‹é™ 15%ï¼Œè¯æ˜ç‰¹å¾è´¨é‡æ‰æ˜¯å…³é”®
2. **é‡ç½®æœºåˆ¶æ˜¯æƒå®œä¹‹è®¡**ï¼šReDo è™½æœ‰æ•ˆï¼Œä½†å¼•å…¥è®­ç»ƒä¸ç¨³å®šæ€§ï¼ˆé”¯é½¿æ³¢åŠ¨ï¼‰ï¼Œä¸”è¶…å‚æ•°æ•æ„Ÿ
3. **è°±å½’ä¸€åŒ–æ˜¯æ›´ä¼˜æ–¹æ¡ˆ**ï¼šä»æ•°å­¦ä¸Šçº¦æŸ Lipschitz å¸¸æ•°ï¼Œæœ‰æ•ˆç¼“è§£ç‰¹å¾ç§©å´©æºƒï¼Œå®ç°ç¨³å®šæ€§ä¸å¯å¡‘æ€§çš„æ›´ä¼˜æƒè¡¡

## ğŸš€ å¿«é€Ÿå¤ç° (Quick Start)

### ç¯å¢ƒé…ç½®

```bash
# 1. åˆ›å»º conda ç¯å¢ƒ
conda create -n rlcourse python=3.10
conda activate rlcourse

# 2. å®‰è£… PyTorch (CUDA 12.1)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# 3. å®‰è£…å…¶ä»–ä¾èµ–
pip install -r requirements.txt
```

### è¿è¡Œå®éªŒ

```bash
# è¿è¡Œ Spectral Norm å®éªŒ (æ¨è)
python train.py -p hyperparams_quick.yaml -n specnorm_experiment

# è¿è¡Œ Baseline å®éªŒ
# ä¿®æ”¹ hyperparams_quick.yaml ä¸­çš„ conditions ä¸º baseline
python train.py -p hyperparams_quick.yaml -n baseline_experiment

# ä»æ–­ç‚¹ç»­è®­
python train.py -p hyperparams_quick.yaml -n <experiment_name> -r

# ç”Ÿæˆå¯¹æ¯”å›¾
python plot_comparison.py
```


## ğŸ“ é¡¹ç›®ç»“æ„ (Project Structure)

```
deep-rl-plasticity/
â”œâ”€â”€ train.py                    # è®­ç»ƒå…¥å£è„šæœ¬
â”œâ”€â”€ plot_comparison.py          # å¯¹æ¯”å›¾ç”Ÿæˆè„šæœ¬
â”œâ”€â”€ hyperparams_quick.yaml      # å¿«é€Ÿå®éªŒé…ç½® (3000 epochs)
â”œâ”€â”€ requirements.txt            # Python ä¾èµ–
â”‚
â”œâ”€â”€ algos/ppo/                  # PPO ç®—æ³•å®ç°
â”‚   â”œâ”€â”€ model.py                # ç½‘ç»œæ¨¡å‹ (å« Spectral Norm æ”¯æŒ)
â”‚   â””â”€â”€ trainer.py              # PPO è®­ç»ƒå™¨
â”‚
â”œâ”€â”€ shared/                     # å…±äº«æ¨¡å—
â”‚   â”œâ”€â”€ modules.py              # ç½‘ç»œç»„ä»¶ (Spectral Norm, Mish ç­‰)
â”‚   â”œâ”€â”€ runner.py               # å®éªŒè¿è¡Œå™¨
â”‚   â””â”€â”€ plotting.py             # ç»˜å›¾å·¥å…·
â”‚
â””â”€â”€ results/                    # å®éªŒç»“æœ
    â”œâ”€â”€ baseline/               # Baseline å®éªŒ (ReLU)
    â”œâ”€â”€ specnorm_experiment/    # Spectral Norm å®éªŒ (æœ€ä½³æ–¹æ³•)
    â”œâ”€â”€ ablation_studies/       # æ¶ˆèå®éªŒ (Mish, Leaky ReLU, RMSNorm, ReDo)
    â””â”€â”€ comparison_figures/     # å¯¹æ¯”å›¾
```

## ğŸ”¬ æ–¹æ³•è¯¦è§£ (Methodology)

### Spectral Normalization (è°±å½’ä¸€åŒ–)

Spectral Normalization é€šè¿‡çº¦æŸæƒé‡çŸ©é˜µçš„è°±èŒƒæ•°ï¼ˆæœ€å¤§å¥‡å¼‚å€¼ï¼‰æ¥ç¨³å®šè®­ç»ƒï¼š

$$W_{SN} = \frac{W}{\sigma(W)}$$

å…¶ä¸­ $\sigma(W)$ æ˜¯æƒé‡çŸ©é˜µ $W$ çš„æœ€å¤§å¥‡å¼‚å€¼ã€‚

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**
- é˜²æ­¢ç‰¹å¾ç§©å´©æºƒï¼ˆFeature Rank Collapseï¼‰
- ç¨³å®šè®­ç»ƒè¿‡ç¨‹ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸
- ä¿æŒç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›å’Œå¯å¡‘æ€§

### å®éªŒè®¾ç½®

| å‚æ•° | å€¼ |
|:-----|:---|
| ç¯å¢ƒ | ProcGen CoinRun |
| ç®—æ³• | PPO |
| è®­ç»ƒè½®æ•° | 3000 epochs |
| ä»»åŠ¡åˆ‡æ¢ç‚¹ | [1000, 2000] |
| éšè—å±‚å¤§å° | 256 |
| å­¦ä¹ ç‡ | 0.0005 |

## ğŸ§ª æ¶ˆèå®éªŒ (Ablation Studies)

æˆ‘ä»¬ç³»ç»Ÿæ€§åœ°æµ‹è¯•äº†å¤šç§æ–¹æ³•ï¼š

| æ–¹æ³• | åŸç† | Test Reward | Dead Units | ç»“æœ |
|:-----|:-----|:-----------:|:----------:|:-----|
| Baseline | åŸå§‹ ReLU | 5.80 | 82.4% | åŸºå‡† |
| Mish | å¹³æ»‘æ¿€æ´»å‡½æ•° | 5.72 | 93.6% | âŒ æ›´å·® |
| Leaky ReLU | è´ŸåŒºé—´ä¿ç•™æ–œç‡ | 4.94 | 0.0% | âŒ æ€§èƒ½ä¸‹é™ |
| LayerNorm | å·¥ä¸šç•Œæ ‡å‡†å½’ä¸€åŒ– | 4.65 | 75.9% | âŒ æ€§èƒ½ä¸‹é™ |
| RMSNorm | è½»é‡çº§å½’ä¸€åŒ– | 4.21 | 67.4% | âŒ æ€§èƒ½æœ€å·® |
| ReDo Reset | å‘¨æœŸæ€§é‡ç½®ç¥ç»å…ƒ | 5.73 | 71.4% | âš ï¸ æ•ˆæœæœ‰é™ |
| **Spectral Norm** | è°±å½’ä¸€åŒ– | **6.96** | **39.5%** | âœ… **æœ€ä½³** |

è¯¦ç»†å®éªŒæ•°æ®ä¿å­˜åœ¨ `results/ablation_studies/` ç›®å½•ä¸‹ã€‚

## ğŸ“š å‚è€ƒæ–‡çŒ® (References)

```bibtex
@article{dohare2024plasticity,
  title={A Study of Plasticity Loss in On-Policy Deep Reinforcement Learning},
  author={Dohare, Shibhansh and others},
  journal={arXiv preprint arXiv:2405.19153},
  year={2024}
}

@inproceedings{miyato2018spectral,
  title={Spectral Normalization for Generative Adversarial Networks},
  author={Miyato, Takeru and others},
  booktitle={ICLR},
  year={2018}
}
```

## ğŸ“„ License

MIT License

---

<p align="center">
  <i>Made with â¤ï¸ for Deep Reinforcement Learning Course Design</i>
</p>
